{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAVqLGocnkJ5",
        "outputId": "a3294e9d-c75f-4575-9a34-d532130bd4b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Best Parameters:\n",
            "Learning Rate (α)    1.000000e-01\n",
            "Lambda (λ)           1.000000e-15\n",
            "Cost                 1.445737e+02\n",
            "R² Score            -1.723585e+00\n",
            "Name: 24, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Q.1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Generate correlated dataset\n",
        "np.random.seed(42)\n",
        "n_samples = 200\n",
        "X_base = np.random.rand(n_samples, 1)\n",
        "X = np.hstack([X_base + np.random.normal(0, 0.01, (n_samples, 1)) for _ in range(7)])\n",
        "true_weights = np.array([5, 4.8, 5.2, 5, 4.9, 5.1, 5])\n",
        "y = X.dot(true_weights) + np.random.normal(0, 0.5, n_samples)\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 2: Ridge Regression with overflow protection\n",
        "def ridge_regression_gradient_descent(X, y, alpha, lamda, epochs=1000):\n",
        "    m, n = X.shape\n",
        "    w = np.zeros(n)\n",
        "    prev_cost = float('inf')\n",
        "\n",
        "    for i in range(epochs):\n",
        "        y_pred = X.dot(w)\n",
        "        error = y_pred - y\n",
        "        cost = (1/(2*m)) * (np.sum(error**2) + lamda * np.sum(w**2))\n",
        "\n",
        "        # Gradient\n",
        "        grad = (1/m) * (X.T.dot(error) + lamda * w)\n",
        "        w -= alpha * grad\n",
        "\n",
        "        # Check for divergence\n",
        "        if np.isnan(cost) or np.isinf(cost) or cost > prev_cost * 1e5:\n",
        "            # If cost explodes, stop early\n",
        "            print(f\"⚠️ Divergence detected (α={alpha}, λ={lamda}), stopping early at epoch {i}\")\n",
        "            break\n",
        "\n",
        "        prev_cost = cost\n",
        "\n",
        "    return w, cost\n",
        "\n",
        "# Step 3: Try different parameters\n",
        "alphas = [0.0001, 0.001, 0.01, 0.1]   # removed large values (1, 10)\n",
        "lambdas = [1e-15, 1e-10, 1e-5, 1e-3, 0, 1, 10, 20]\n",
        "\n",
        "results = []\n",
        "\n",
        "for alpha in alphas:\n",
        "    for lamda in lambdas:\n",
        "        w, cost = ridge_regression_gradient_descent(X_scaled, y, alpha, lamda)\n",
        "        y_pred = X_scaled.dot(w)\n",
        "        if np.any(np.isnan(y_pred)) or np.any(np.isinf(y_pred)):\n",
        "            continue  # skip invalid runs\n",
        "        r2 = r2_score(y, y_pred)\n",
        "        results.append((alpha, lamda, cost, r2))\n",
        "\n",
        "# Step 4: Find best parameters\n",
        "results_df = pd.DataFrame(results, columns=['Learning Rate (α)', 'Lambda (λ)', 'Cost', 'R² Score'])\n",
        "best_params = results_df.loc[results_df['R² Score'].idxmax()]\n",
        "\n",
        "print(\"✅ Best Parameters:\")\n",
        "print(best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.2\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ML/Hitters.csv\")\n",
        "\n",
        "# (a) Pre-process data\n",
        "df = df.dropna(subset=['Salary'])\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# (b) Separate input and output, perform scaling\n",
        "X = df.drop('Salary', axis=1)\n",
        "y = df['Salary']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# (c) Fit models\n",
        "ridge = Ridge(alpha=0.5748)\n",
        "lasso = Lasso(alpha=0.5748)\n",
        "linear = LinearRegression()\n",
        "\n",
        "models = {'Linear': linear, 'Ridge': ridge, 'Lasso': lasso}\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    results[name] = {'R2': r2_score(y_test, y_pred), 'MSE': mean_squared_error(y_test, y_pred)}\n",
        "\n",
        "# (d) Display results\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krnFSFb6s0mN",
        "outputId": "fc49318d-00f8-4869-d154-6b0ea442612b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              R2            MSE\n",
            "Linear  0.290745  128284.345497\n",
            "Ridge   0.300036  126603.902644\n",
            "Lasso   0.299286  126739.568991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.185e+04, tolerance: 4.367e+03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.3\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ML/housing.csv\").dropna()\n",
        "\n",
        "# Convert categorical columns (if any) to numeric\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Use last column as target\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Ridge and Lasso with Cross Validation\n",
        "ridgecv = RidgeCV(alphas=[0.1, 1, 10, 100], cv=5)\n",
        "lassocv = LassoCV(alphas=[0.1, 1, 10, 100], cv=5, max_iter=10000)\n",
        "\n",
        "ridgecv.fit(X_train, y_train)\n",
        "lassocv.fit(X_train, y_train)\n",
        "\n",
        "ridge_pred = ridgecv.predict(X_test)\n",
        "lasso_pred = lassocv.predict(X_test)\n",
        "\n",
        "print(\"Ridge Best Alpha:\", ridgecv.alpha_)\n",
        "print(\"Ridge R2:\", r2_score(y_test, ridge_pred))\n",
        "print(\"Lasso Best Alpha:\", lassocv.alpha_)\n",
        "print(\"Lasso R2:\", r2_score(y_test, lasso_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQMeOKDVo_f1",
        "outputId": "125f0e60-1bd5-4eff-fe6b-077c19ba3b1d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Best Alpha: 0.1\n",
            "Ridge R2: 0.0\n",
            "Lasso Best Alpha: 100.0\n",
            "Lasso R2: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.4\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# One-vs-Rest strategy\n",
        "model_ovr = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
        "model_ovr.fit(X_train, y_train)\n",
        "y_pred_ovr = model_ovr.predict(X_test)\n",
        "\n",
        "# One-vs-One strategy\n",
        "model_ovo = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
        "model_ovo.fit(X_train, y_train)\n",
        "y_pred_ovo = model_ovo.predict(X_test)\n",
        "\n",
        "print(\"OvR Accuracy:\", accuracy_score(y_test, y_pred_ovr))\n",
        "print(\"OvO Accuracy:\", accuracy_score(y_test, y_pred_ovo))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OC0n6k8sJD1",
        "outputId": "a4f33ce4-f372-48b9-c3b5-e9f195aaf347"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvR Accuracy: 0.9666666666666667\n",
            "OvO Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}